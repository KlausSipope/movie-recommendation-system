##### Name: Mr. Klaus Sipope
##### Position: Data Scientist
##### Name of Program: Movie Recommendation System.ipynb
##### Description: Build a Movie Recommendation System with Machine Learning and Python
##### Date first written: Thu, 19-Oct-2023
##### Date last updated: Fri, 10-Nov-2023
# Introduction
In this notebook, we will implement a few recommendation algorithms such as content based, popularity based and collaborative filtering and try to build an ensemble of these models to come up with our final recommendation system. 

With us, we have two MovieLens datasets: 
- The Full Dataset: it consists of 26,000,000 ratings and 750,000 tag applications applied to 45,000 movies by 270,000 users. It includes tag genome data with 12 million relevance scores across 1,100 tags. 
- The Small Dataset: it comprises of 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users.
## Imports
%matplotlib inline
from IPython.display import Image, HTML
import json
import datetime
import ast
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from ast import literal_eval
from wordcloud import WordCloud, STOPWORDS
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet
from surprise import Reader, Dataset, SVD, NormalPredictor, BaselineOnly, KNNBasic, NMF
from surprise.model_selection import cross_validate, KFold ,GridSearchCV , RandomizedSearchCV

import warnings; warnings.simplefilter('ignore')
df = pd.read_csv('D:\APU\Semester 3\CP2\Dataset\movies_dataset\movies_metadata.csv')
df.head().transpose()
## Understanding the dataset
df.columns
df.shape
df.info()
The dataset contains a total of 45,466 movies with 24 features. Majority of the features have very few NaN values (apart from homepage and tagline). We will attempt at cleaning this dataset to a form suitable for analysis in the preprocessing section.
# Data Preprocessing
In this section will be discussed the preprocessing of the movies_metadata dataset.
### Feature Dropping
df = df.drop(['imdb_id'], axis=1)
df[df['original_title'] != df['title']][['title', 'original_title']].head()
The feature 'original_title' refers to the title of a movie in the native language in which that movie was produced. It is preferable to use the translated (english title) in this analysis and hence, drop column original_title. By looking at the original_language feature, we will be able to deduce if a movie is a foreign language film, so no important information is lost in doing so.
df = df.drop('original_title', axis=1)
### Revenue & Budget Handling
df[df['revenue'] == 0].shape
It is found that the column 'revenue' contains a lot of zero as value. Many movies lacked revenue information, these entries will be replaced by NaN, marking them for special attention to avoid skewing subsequent analyses and computations.
df['revenue'] = df['revenue'].replace(0, np.nan)
The column 'budget' contains some unclean values that makes Pandas assign it as a generic object. To solve this, we convert the column into a numeric variable and replace all the non-numeric values with NaN. Finally, as with 'revenue' feature, we will convert all the values of zero into NaN to indicate the absence of information regarding budget.
df['budget'] = pd.to_numeric(df['budget'], errors='coerce')
df['budget'] = df['budget'].replace(0, np.nan)
df[df['budget'].isnull()].shape
### Feature Engineering
Two significant new features, ‘year’ and 'return’, will be created to improve the analysis of the dataset. 
- year: will be obtained by extracting the release year from the ‘release_date’ column, providing a categorical temporal attribute. 

- return: will be obtained from the calculation of the ratio of revenue to budget. It was created to evaluate the financial performance of movies more accurately.
df['return'] = df['revenue'] / df['budget']
df[df['return'].isnull()].shape
df['year'] = pd.to_datetime(df['release_date'], errors='coerce').apply(lambda x: 
                                                                       str(x).split('-')[0] if x != np.nan else np.nan)
### Additional Cleansing
df['adult'].value_counts()
The ‘adult’ column will be dropped from the dataset due to the fact that it contains mainly ‘False’ entries, with minimal counts of ‘True’ and some irrelevant text data that are considered as input errors.
df = df.drop('adult', axis=1)
# EDA
### Title and Overview Wordclouds
To identify prevailing themes and popular words that could significantly impact the recommendation process, an EDA of the features 'title' and 'overview' is realized. By understanding the recurrent themes and commonly used terms, the model can be fine-tuned to capture and prioritize these prevalent aspects in movie suggestions.
df['title'] = df['title'].astype('str')
df['overview'] = df['overview'].astype('str')
title_corpus = ' '.join(df['title'])
overview_corpus = ' '.join(df['overview'])
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(title_corpus)
plt.figure(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.show()
It can be observed that the word Love is the word that is mostly used in movie titles. Girl, Day and Man are also among the most commonly used words.
overview_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(overview_corpus)
plt.figure(figsize=(16,8))
plt.imshow(overview_wordcloud)
plt.axis('off')
plt.show()
The most commonly used word in movie titles is Life. One and Find are also popular in Movie Blurbs.
### Popularity, Vote Average and Vote Count
We will be working with metrics that TMDB users have sent us in this section. Along with other numerical features like budget and revenue, we will attempt to better comprehend the popularity, vote average, and vote count features and try to deduce any relationships between them.

def clean_numeric(x):
    try:
        return float(x)
    except:
        return np.nan
df['popularity'] = df['popularity'].apply(clean_numeric).astype('float')
df['vote_count'] = df['vote_count'].apply(clean_numeric).astype('float')
df['vote_average'] = df['vote_average'].apply(clean_numeric).astype('float')
df['popularity'].describe()
sns.distplot(df['popularity'].fillna(df['popularity'].median()))
plt.show()
df['popularity'].plot(logy=True, kind='hist')
The Popularity score shows an extreme skewness, with a mean of only 2.9 and maximum values up to 547—nearly 1800% higher than the mean. However, the distribution map shows that practically all films have a popularity score below 10 (3.678902 is the 75th percentile).

### Relationship between popularity and vote average
sns.jointplot(x='vote_average', y='popularity', data=df)
Interestingly, there appears to be no discernible correlation as the Pearson Coefficient of the two previously mentioned features is only 0.097. Put differently, the average of popularity and votes and independent quantities. Finding out how TMDB rates the popularity of each of their films on a numerical scale would be interesting.

sns.jointplot(x='vote_average', y='vote_count', data=df)
The correlation between the features Vote Count and Vote Average is very small. A film does not always indicate that it is good just because it has received a lot of votes.
# Modelling
## Simple Recommendation System
##### This model's implementation is relatively simple. 
##### All we have to do is sort our movies by rating and popularity and display the top films on our list. 
##### As an extra step, we can include a genre argument to get the top movies in a specific genre.
df['genres'] = df['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
### Top Movie Chart Generation
vote_counts = df[df['vote_count'].notnull()]['vote_count'].astype('int')
vote_averages = df[df['vote_average'].notnull()]['vote_average'].astype('int')
C = vote_averages.mean()
C
m = vote_counts.quantile(0.95)
m
df['year'] = pd.to_datetime(df['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)
qualified = df[(df['vote_count'] >= m) & (df['vote_count'].notnull()) & (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']]
qualified['vote_count'] = qualified['vote_count'].astype('int')
qualified['vote_average'] = qualified['vote_average'].astype('int')
qualified.shape
Therefore, for a movie to be on the chart, it has to have at least 434 votes on TMDB. It is also seen that the average rating for a movie on TMDB is 5.244 on a scale of 10. The number of movies that can appear on the chart is 2274.
### Weighted Rating
def weighted_rating(x):
    v = x['vote_count']
    R = x['vote_average']
    return (v/(v+m) * R) + (m/(m+v) * C)
qualified['wr'] = qualified.apply(weighted_rating, axis=1)
qualified = qualified.sort_values('wr', ascending=False).head(250)
qualified.head(15)
### Genre-Based Recommendations
In this section, we are going to construct a function that builds charts for particular genres. Relaxing the default conditions to the 85th percentile instead of 95, will help us achieve our aim.
s = df.apply(lambda x: pd.Series(x['genres']),axis=1).stack().reset_index(level=1, drop=True)
s.name = 'genre'
gen_df = df.drop('genres', axis=1).join(s)
def build_chart(genre, percentile=0.85):
    df = gen_df[gen_df['genre'] == genre]
    vote_counts = df[df['vote_count'].notnull()]['vote_count'].astype('int')
    vote_averages = df[df['vote_average'].notnull()]['vote_average'].astype('int')
    C = vote_averages.mean()
    m = vote_counts.quantile(percentile)
    
    qualified = df[(df['vote_count'] >= m) & (df['vote_count'].notnull()) & (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity']]
    qualified['vote_count'] = qualified['vote_count'].astype('int')
    qualified['vote_average'] = qualified['vote_average'].astype('int')
    
    qualified['wr'] = qualified.apply(lambda x: (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C), axis=1)
    qualified = qualified.sort_values('wr', ascending=False).head(250)
    
    return qualified
Let's test the function
build_chart('Action').head(15)
## Content-Based Filtering
For the building of this model, we will be using a subset of all the movies available to us due to limiting computing power at our disposal.
links_small = pd.read_csv('D:\APU\Semester 3\CP2\Dataset\movies_dataset\links_small.csv')
links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')
df = df.drop([19730, 29503, 35587])
df['id'] = df['id'].astype('int')
sdf = df[df['id'].isin(links_small)]
sdf.shape
There are 9099 movies avaiable in the small movies metadata dataset which is 5 times smaller than the original dataset.
### CBF Based on Movie Description Using Cosine Similarity
sdf['tagline'] = sdf['tagline'].fillna('')
sdf['description'] = sdf['overview'] + sdf['tagline']
sdf['description'] = sdf['description'].fillna('')
tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=1, stop_words='english')
tfidf_matrix = tf.fit_transform(sdf['description'])
tfidf_matrix.shape
Since the TF-IDF Vectorizer was used, calculating the Dot Product will directly give the Cosine Similarity score. Therefore, sklearn's linear_kernel will be used instead of cosine_similarities since it is much faster.
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
cosine_sim[0]
Now, there is a pairwise cosine similarity matrix for all the movies in the dataset. The next step will be to build a function that shows the 30 most similar movies based on the cosine similarity score.
sdf = sdf.reset_index()
titles = sdf['title']
indices = pd.Series(sdf.index, index=sdf['title'])
def get_recommendations(title):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:31]
    movie_indices = [i[0] for i in sim_scores]
    return titles.iloc[movie_indices]
Let's test the function
get_recommendations('The Dark Knight').head(10)
get_recommendations('Toy Story').head(10)
We see that for Toy Story, the system is able to identify it as a movie from the franchise Toy Story and therefore recommend other Toy Story films as its top recommendations. But unfortunately, that is all this system is able to do at the moment. This doesn't really help much because it ignores crucial elements that affect a movie's popularity and rating, like cast, crew, director, and genre.

Someone who liked Toy Story probably likes it more because it's a pixar movie produced by director John Lasseter and would not like that much "What's Up, Tiger Lily?" or every other movie in the list that is not from pixar and produced by John Lasseter.

Therefore, more suggestive metadata are going to be used. In the next part will be built a more advanced RS that takes genre, keywords, cast and crew into consideration.
### CBF Based on Metadata
credits = pd.read_csv('D:\APU\Semester 3\CP2\Dataset\movies_dataset\credits.csv')
keywords = pd.read_csv('D:\APU\Semester 3\CP2\Dataset\movies_dataset\keywords.csv')
keywords['id'] = keywords['id'].astype('int')
credits['id'] = credits['id'].astype('int')
df['id'] = df['id'].astype('int')
df.shape
df = df.merge(credits, on='id')
df = df.merge(keywords, on='id')
sdf = df[df['id'].isin(links_small)]
sdf.shape
Now that we have cast, crew, genres and credits, all in one dataframe, here is how we are going to proceed:

- Crew: Only the director will be picked since the others don't contribute that much to the feel of the movie.

- Cast: Cast selection is a little trickier. Minor roles and lesser-known performers don't actually change people's perceptions of a film. We can only choose the main characters and the actors who play them. We shall select the top three actors listed in the credits at random.
sdf['cast'] = sdf['cast'].apply(literal_eval)
sdf['crew'] = sdf['crew'].apply(literal_eval)
sdf['keywords'] = sdf['keywords'].apply(literal_eval)
sdf['cast_size'] = sdf['cast'].apply(lambda x: len(x))
sdf['crew_size'] = sdf['crew'].apply(lambda x: len(x))
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return np.nan
sdf['director'] = sdf['crew'].apply(get_director)
sdf['cast'] = sdf['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
sdf['cast'] = sdf['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)
sdf['keywords'] = sdf['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
The approach to build the RS is going to be a bit clumsy. The goal here isto compile a metadata dump of all the films, including the principal actors, director, genres, and keywords. Then, just like we did with the Description RS, we will utilize a Count Vectorizer to generate our count matrix. The subsequent steps are the same as in the previous section: we compute the cosine similarity and return the most comparable movies.

Hese are the steps we are going to follow to prepare the features genres and credits:

- Remove spaces and change all of our features to lowercase. In this manner, Johnny Depp and Johnny Galecki won't be confused by the system.

- To give the director additional weight in relation to the full cast, mention it three times.

sdf['cast'] = sdf['cast'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])
sdf['director'] = sdf['director'].astype('str').apply(lambda x: str.lower(x.replace(" ", "")))
sdf['director'] = sdf['director'].apply(lambda x: [x,x, x])
#### Preprocessing of keywords
s = sdf.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)
s.name = 'keyword'
s = s.value_counts()
s[:5]
Frequencies of keywords range from 1 to 610. Keywords that appear only once are not useful to us. Consequently, it is safe to remove these. Ultimately, each word will be reduced to its stem, treating terms like "dogs" and "dog" equally.

s = s[s > 1]
stemmer = SnowballStemmer('english')
stemmer.stem('dogs')
def filter_keywords(x):
    words = []
    for i in x:
        if i in s:
            words.append(i)
    return words
sdf['keywords'] = sdf['keywords'].apply(filter_keywords)
sdf['keywords'] = sdf['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])
sdf['keywords'] = sdf['keywords'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])
sdf['soup'] = sdf['keywords'] + sdf['cast'] + sdf['director'] + sdf['genres']
sdf['soup'] = sdf['soup'].apply(lambda x: ' '.join(x))
count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=1, stop_words='english')
count_matrix = count.fit_transform(sdf['soup'])
cosine_sim = cosine_similarity(count_matrix, count_matrix)
sdf = sdf.reset_index()
titles = sdf['title']
indices = pd.Series(sdf.index, index=sdf['title'])
Let's reuse the get_recommendations function that we built earlier. Since the cosine similarity scores have changed, it is expected to have different (and probably better) results. Let us check for The Toy Story again and see what recommendations we get this time.
get_recommendations('Toy Story').head(10)
get_recommendations('Toy Story').head(10)
This time, the results are much more satisfying. The recommendations seems to have recognized other pixar movies produced by john Lasseter
#### Popularity and Ratings
One thing that is noticed about the RS we built, is that it recommends movies regardless of ratings and popularity. It is true that Luxo Jr. has a lot of similarity with Toy Story but it wasn't that good.

Therefore, a mechanism will be added to remove movies with bad ratings and return movies that have a better critical response.

The top 25 movies will be taken, based on similarity scores, and the vote of the 60th percentile movie will be calculated. Then, we will use the IMDB method to determine each movie's weighted rating, just as we did in the Simple Recommender section, using this as the value of m.
def improved_recommendations(title):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:26]
    movie_indices = [i[0] for i in sim_scores]
    
    movies = sdf.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year']]
    vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')
    vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')
    C = vote_averages.mean()
    m = vote_counts.quantile(0.60)
    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]
    qualified['vote_count'] = qualified['vote_count'].astype('int')
    qualified['vote_average'] = qualified['vote_average'].astype('int')
    qualified['wr'] = qualified.apply(weighted_rating, axis=1)
    qualified = qualified.sort_values('wr', ascending=False).head(10)
    return qualified
improved_recommendations('Toy Story')
## Collaborative Filtering Using Surprise Library
The CBF model we built, suffers from some severe limitations. It can only suggest movies which are similar to a certain movie, which means it is not capable of capturing tastes and providing recommendations across genres.

Also, the model we built is not really personalized in that it doesn't capture the personal tastes and biases of a user. Anyone using that model for recommendations based on a movie will receive the same recommendations for that movie, regardless of who they are.

Therefore, in this section, we will use a technique called Collaborative Filtering to make recommendations to users.

We won't implement CF from scratch. Instead, we will use the Surprise library that used extremely powerful algorithms and provides great recommendations.
reader = Reader(rating_scale=(1, 5))
ratings = pd.read_csv('D:/APU/Semester 3/CP2/Dataset/movies_dataset/ratings_small.csv')
ratings.head()
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)
#### Different Prediction Algorithms
algo = NormalPredictor()
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
algo = SVD()
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
algo = KNNBasic(k=20)
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
algo = KNNBasic(sim_options={'user_based': False} , k=20) 
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
algo = NMF()
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
Parameters:

- n_factors – The number of factors. Default is 100.
- n_epochs – The number of iteration of the SGD procedure. Default is 20.
- init_mean – The mean of the normal distribution for factor vectors initialization. Default is 0.
- init_std_dev – The standard deviation of the normal distribution for factor vectors initialization. Default is 0.1.
- lr_all – The learning rate for all parameters. Default is 0.005.
- reg_all – The regularization term for all parameters. Default is 0.02.
### Predictions
sup_train = data.build_full_trainset()
algo = SVD(n_factors = 200 , lr_all = 0.005 , reg_all = 0.02 , n_epochs = 40 , init_std_dev = 0.05)
algo.fit(sup_train)
def prediction_algo(uid = None , iid = None):
    predictions = []
    if uid is None:
        for ui in sup_train.all_users():
            predictions.append(algo.predict(ui, iid, verbose = False))
        return predictions
    
    if iid is None:
        for ii in sup_train.all_items():
            ii = sup_train.to_raw_iid(ii)
            predictions.append(algo.predict(uid, ii, verbose = False))
        return predictions
    return predictins.append(algo.predict(uid,iid,verbose = False))
predictions = prediction_algo(uid = 112)
predictions.sort(key=lambda x: x.est, reverse=True)
print('#### Best Recommanded Movies are ####')
for pred in predictions[:21]:
#     print('Movie -> {} with Score-> {}'.format(sup_train.to_raw_iid(pred.iid) , pred.est))
    print('Movie -> {} with Score-> {}'.format(pred.iid, pred.est))
## Hybrid Filtering
After building the CBF and CF models, we are going to build a simple hybrid recommendation system that brings together techniques that were implemented in both CBF and CF. This is how it is going to work:

- Input: User ID and the Title of a Movie
- Output: Similar movies sorted on the basis of expected ratings by that particular user.
def convert_int(x):
    try:
        return int(x)
    except:
        return np.nan
id_map = pd.read_csv('D:/APU/Semester 3/CP2/Dataset/movies_dataset/links_small.csv')[['movieId', 'tmdbId']]
id_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)
id_map.columns = ['movieId', 'id']
id_map = id_map.merge(sdf[['title', 'id']], on='id').set_index('title')
indices_map = id_map.set_index('id')
def hybrid(userId, title):
    idx = indices.loc[title]
    tmdbId = id_map.loc[title]['id']
    movie_id = id_map.loc[title]['movieId']
    
    sim_scores = list(enumerate(cosine_sim[int(idx)]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:26]
    movie_indices = [i[0] for i in sim_scores]
    
    movies = sdf.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year', 'id']]
    movies['est'] = movies['id'].apply(lambda x: algo.predict(userId, indices_map.loc[x]['movieId']).est)
    movies = movies.sort_values('est', ascending=False)
    return movies.head(10)
hybrid(500, 'Toy Story')
hybrid(500, 'The Godfather')
It can be seen that for our hybrid RS, we get different recommendations for different users although the movie is the same. Hence, the recommendations are more personalized and tailored towards particular users.
import pickle
pickle.dump(sdf,open('sdf.pkl','wb'))
pickle.dump(indices,open('indices.pkl','wb'))
pickle.dump(id_map,open('id_map.pkl','wb'))
pickle.dump(indices_map,open('indices_map.pkl','wb'))
pickle.dump(cosine_sim,open('cosine_sim.pkl','wb')) 
pickle.dump(indices.to_dict(),open('title.pkl','wb'))
sdf['poster_path']
sdf['id']
cosine_sim
